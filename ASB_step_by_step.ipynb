{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t oceans16 -f roboto -fs 12 -cellw 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important parameters\n",
    "target_name='default_cross12'\n",
    "time_name='period'\n",
    "event_rate_name='BR'\n",
    "event_name='Bad'\n",
    "prob_event='PD'\n",
    "nonevent_name='Good'\n",
    "share_name='Share'\n",
    "variable_name='Variable'\n",
    "grp_name='GRP'\n",
    "all_name='All'\n",
    "logit_name='Logit'\n",
    "condition_name='Condition'\n",
    "time_report_name='Time'\n",
    "intercept_name='Intercept'\n",
    "score_name='Score'\n",
    "beta_name='Beta'\n",
    "fbeta_name='FBeta'\n",
    "id_row='aid'\n",
    "gini_train='Gini train'\n",
    "gini_test='Gini test'\n",
    "delta_gini='R. Gini'\n",
    "estimation='Estimation'\n",
    "pvalue='P-value'\n",
    "max_pvalue='Max p-value'\n",
    "max_vif='Max VIF'\n",
    "max_con_index='Max Con Index'\n",
    "max_pearson='Max Pearson'\n",
    "nnegative_betas='N negative betas'\n",
    "wald_test='Wald test'\n",
    "degree_free='Degrees of freedom'\n",
    "std_err='Standard error'\n",
    "bad_share='Bad share'\n",
    "good_share='Good share'\n",
    "INV='Infomration Value'\n",
    "PSI='Population Stability Index'\n",
    "PSI_tar='Population Stability Index for bads'\n",
    "share_name_test='Share test'\n",
    "bad_share_test='Bad share test'\n",
    "type_name='Type'\n",
    "percent_missing='Missing percent'\n",
    "count_unique='Number of distinct'\n",
    "event_value='outstanding_bad'\n",
    "all_value='outstanding'\n",
    "event_rate_name_value='BRBal'\n",
    "nonevent_name_value='Balance good'\n",
    "share_name_value='Balance share'\n",
    "mode_name='Mode'\n",
    "mode_pname='P. mode'\n",
    "type_name='Type'\n",
    "\n",
    "\n",
    "#Bining for numerical variables\n",
    "ncategories_int=5 #zmiana\n",
    "minimum_share_int=0.03\n",
    "symbol_missing='Missing'\n",
    "\n",
    "#Bining for character variables\n",
    "symbol_other='<OTHERS>'\n",
    "#minimum share of unique value\n",
    "minimum_share_unique=0.15\n",
    "#minimum_share_unique=0.03 #zmiana\n",
    "#maximal number of bins\n",
    "ncategories_nom=4 #zmiana\n",
    "\n",
    "#option for response model\n",
    "# category_order=True\n",
    "#option for risk model\n",
    "category_order=False\n",
    "description_results = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# ef = pd.ExcelFile('abt_app_PD_INS.xlsx')\n",
    "# ef = pd.ExcelFile('abt_app.xls')\n",
    "# df = ef.parse('Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sas('data/abt_app.sas7bdat', encoding='LATIN2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['app_char_job_code'][0]))\n",
    "print(df['app_char_job_code'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[('197501'<=df['period']) & (df['period']<='198712') & (df['product']=='css') & (df['decision']=='A')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[intercept_name]=1\n",
    "df[event_value]=df['app_loan_amount']*df[target_name]\n",
    "df[all_value]=df['app_loan_amount']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we have mising values in target variable?\n",
    "one=df[target_name]\n",
    "one[one.isnull()==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping mising values in target variable\n",
    "df_notempty=df.dropna(subset=[target_name])\n",
    "one2=df_notempty[target_name]\n",
    "one2[one2.isnull()==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of variables\n",
    "#vars=[var for var in list(df) if var[0:3].lower() in ['app','act']] #zmiana\n",
    "vars=[var for var in list(df) if var[0:3].lower() in ['app','act','agr','ags']]\n",
    "vars_target_id=vars+[target_name]+[time_name]+[intercept_name]+[event_value]+[all_value]+[id_row]\n",
    "#vars_target_id\n",
    "\n",
    "#splitting into numeric and character variables\n",
    "varsc=list(df[vars].select_dtypes(include='object'))\n",
    "varsn=list(df[vars].select_dtypes(include='number'))\n",
    "\n",
    "#print(varsc, varsn)\n",
    "#vars_target_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting for train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_notempty[vars_target_id], random_state = 1234, test_size=0.35)\n",
    "print(train.shape, test.shape)\n",
    "#train.head() \n",
    "#train[train[target_name].isnull()==True].head()\n",
    "#test[test[target_name].isnull()==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bining for numerical variables\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "# remove_list = []\n",
    "labsn = {} # binns lists\n",
    "\n",
    "for feature in varsn:\n",
    "    #feature='app_income'\n",
    "    miss_share=train[feature].isnull().sum()/train[feature].shape[0]\n",
    "    miss_share=1-miss_share\n",
    "    if miss_share<=0.00001:\n",
    "        miss_share=1\n",
    "    minimum_share=minimum_share_int/miss_share\n",
    "    if minimum_share>0.5:\n",
    "        minimum_share=0.5\n",
    "    if minimum_share<minimum_share_int:\n",
    "        minimum_share=minimum_share_int\n",
    "    df_two_col=train[[target_name,feature]].dropna(subset=[feature]).copy()\n",
    "    # binns list with [min,max]\n",
    "    bins = [-np.inf, np.inf]\n",
    "    # get Tree classifier - check if we need another parameters !!\n",
    "    clf = tree.DecisionTreeClassifier(\n",
    "        max_leaf_nodes=ncategories_int,\n",
    "        min_weight_fraction_leaf=minimum_share,\n",
    "        random_state=1234) \n",
    "    # fit tree\n",
    "    clf.fit(df_two_col[feature].values.reshape(-1, 1), df_two_col[target_name])\n",
    "    # get tresholds and remove empty\n",
    "    thresh = [round(s, 3) for s in clf.tree_.threshold if s != -2]  \n",
    "    # add tresholds to binns\n",
    "    bins = bins + thresh \n",
    "    bins=sorted(bins)\n",
    "    if train[feature].isnull().sum()/train[feature].shape[0] > minimum_share_int:\n",
    "        bins=bins + [symbol_missing]\n",
    "    labsn[feature]=bins\n",
    "    \n",
    "\n",
    "#labsn['app_number_of_children']=[-np.inf, 1, 1, 2, np.inf] #todo\n",
    "labsn #zmiana odkomentowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bining for character variables\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# remove_list = []\n",
    "labsc = {} # binns lists\n",
    "\n",
    "for feature in varsc:\n",
    "    df_two_col1=pd.DataFrame(train.groupby(feature)[target_name].count()/train.shape[0])\n",
    "    df_two_col2=pd.DataFrame(train.groupby(feature)[target_name].mean())\n",
    "    df_two_col=df_two_col2\n",
    "    df_two_col['share']=df_two_col1[target_name]\n",
    "    df_two_col = df_two_col.loc[df_two_col['share'] > minimum_share_unique]\n",
    "    ncategoriesv=min(ncategories_nom,df_two_col.shape[0])\n",
    "    if ncategoriesv>=2:\n",
    "        #todo affinity='euclidean'\n",
    "        cluster = AgglomerativeClustering(n_clusters=ncategoriesv, affinity='euclidean', linkage='ward')\n",
    "        cluster.fit_predict(df_two_col[[target_name]])\n",
    "        df_two_col['cluster']=cluster.labels_.reshape(-1,1)\n",
    "    else:\n",
    "        df_two_col['cluster']=0\n",
    "    bins=df_two_col[['cluster']]\n",
    "    if df_two_col['share'].sum() < (1-minimum_share_unique):\n",
    "        bins.loc[symbol_other]=-1\n",
    "        bins['cluster']=bins['cluster']+1\n",
    "    bins=bins.sort_values(by='cluster')\n",
    "    bins=bins.reset_index()\n",
    "    labsc[feature]=bins\n",
    "    \n",
    "#labsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRP variables for numerical variables\n",
    "feature_intervalsn = {}\n",
    "for feature in varsn:\n",
    "    intervals = []\n",
    "    tekst=''\n",
    "    for i in range(len(labsn[feature])):\n",
    "        if i==0 and labsn[feature][i]==-np.inf and labsn[feature][i+1]!=np.inf:\n",
    "            tekst=feature+' < '+str(labsn[feature][i+1])\n",
    "            intervals=intervals + [tekst]        \n",
    "        if i==0 and labsn[feature][i]==-np.inf and labsn[feature][i+1]==np.inf:\n",
    "            tekst=feature+' <> '+symbol_missing\n",
    "            intervals=intervals + [tekst]        \n",
    "        if i>0 and labsn[feature][i-1]!=-np.inf and labsn[feature][i]==np.inf:\n",
    "            tekst=str(labsn[feature][i-1])+' <= '+feature\n",
    "            intervals=intervals + [tekst]        \n",
    "        if i>0 and labsn[feature][i-1]!=-np.inf and labsn[feature][i]!=np.inf and labsn[feature][i]!=symbol_missing:\n",
    "            tekst=str(labsn[feature][i-1])+' <= '+feature+' < '+str(labsn[feature][i])\n",
    "            intervals=intervals + [tekst]        \n",
    "        if labsn[feature][i]==symbol_missing:\n",
    "            tekst=feature+' = '+symbol_missing\n",
    "            intervals=intervals + [tekst]\n",
    "    feature_intervalsn[feature]=intervals\n",
    "        \n",
    "#feature_intervalsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRP variables for character variables\n",
    "feature_intervalsc = {}\n",
    "for feature in varsc:\n",
    "    intervals = []\n",
    "    tekst=''\n",
    "    for i in range(labsc[feature].shape[0]):\n",
    "        if i==0:\n",
    "            tekst=labsc[feature][feature][i]\n",
    "        if i>0:\n",
    "            if labsc[feature]['cluster'][i-1]==labsc[feature]['cluster'][i]:\n",
    "                tekst=tekst+', '+labsc[feature][feature][i]\n",
    "            else:\n",
    "                intervals=intervals + [tekst]\n",
    "                tekst=labsc[feature][feature][i]\n",
    "        if i+1==labsc[feature].shape[0]:\n",
    "            intervals=intervals + [tekst]\n",
    "                    \n",
    "    feature_intervalsc[feature]=intervals\n",
    "        \n",
    "#feature_intervalsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRP variables in train dataset\n",
    "train_grp=train.copy()\n",
    "nnn=0\n",
    "for feature in varsn:\n",
    "    nnn+=1\n",
    "    print(nnn,feature)\n",
    "    def grp(x):\n",
    "        res=np.NaN\n",
    "        for i in range(len(labsn[feature])):\n",
    "            if i==0 and labsn[feature][i]==-np.inf and labsn[feature][i+1]!=np.inf and x<labsn[feature][i+1]:                \n",
    "                    res=i            \n",
    "            if i==0 and labsn[feature][i]==-np.inf and labsn[feature][i+1]==np.inf and (-np.inf<=x<=np.inf):            \n",
    "                    res=i\n",
    "            if i>0 and labsn[feature][i-1]!=-np.inf and labsn[feature][i]==np.inf and labsn[feature][i-1]<=x:            \n",
    "                    res=i-1\n",
    "            if i>0 and labsn[feature][i-1]!=-np.inf and labsn[feature][i]!=np.inf and labsn[feature][i]!=symbol_missing and labsn[feature][i-1]<=x<labsn[feature][i+1]:        \n",
    "                    res=i-1\n",
    "            if labsn[feature][i]==symbol_missing and math.isnan(x):\n",
    "                    res=i-1\n",
    "        return res\n",
    "    train_grp[feature]=train_grp[feature].apply(grp)\n",
    "    \n",
    "for feature in varsc:\n",
    "    nnn+=1\n",
    "    print(nnn,feature)\n",
    "    def grp(x):\n",
    "        res=np.NaN\n",
    "        if type(x) != str:\n",
    "            x=str('')\n",
    "        if labsc[feature][feature][0]==symbol_other:\n",
    "            res=0\n",
    "        for i in range(labsc[feature].shape[0]):\n",
    "            if x==labsc[feature][feature][i] and labsc[feature][feature][i]!=symbol_other:\n",
    "                res=labsc[feature]['cluster'][i]\n",
    "        return res\n",
    "    train_grp[feature]=train_grp[feature].apply(grp)\n",
    "    \n",
    "train_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Big_scorecard\n",
    "Big_scorecard=pd.DataFrame()\n",
    "sum=train_grp.shape[0]\n",
    "sum_bad=train_grp[target_name].sum()\n",
    "sum_good=sum-sum_bad\n",
    "\n",
    "for feature in varsn:\n",
    "    fin=pd.DataFrame()\n",
    "    sss=pd.DataFrame()\n",
    "    ddd=pd.DataFrame()\n",
    "    sss=pd.DataFrame(train_grp.groupby(feature).agg({target_name:['sum','count']}))\n",
    "    sss=pd.DataFrame(sss[target_name])\n",
    "    sss=sss.reset_index()\n",
    "    sss=sss.rename(columns={\"sum\": event_name, \"count\": all_name, feature:grp_name})\n",
    "    sss[nonevent_name]=sss[all_name]-sss[event_name]\n",
    "    sss[event_rate_name]=sss[event_name]/sss[all_name]\n",
    "    sss[logit_name]=np.log((sss[event_name]+0.0001) / (sss[nonevent_name]+0.0001))\n",
    "    sss[share_name]=sss[all_name]/sum\n",
    "    sss[variable_name]=feature    \n",
    "    ddd=pd.DataFrame({grp_name:range(len(feature_intervalsn[feature])),condition_name:feature_intervalsn[feature]})\n",
    "    fin=pd.merge(ddd, sss, on=grp_name)\n",
    "    fin=fin[[variable_name,condition_name,event_rate_name,share_name,all_name,event_name,nonevent_name,logit_name,grp_name]]\n",
    "    fin=fin.sort_values(by=[event_rate_name],ascending=category_order)\n",
    "    fin=fin.reset_index(drop=True)\n",
    "    fin[grp_name]=fin.index\n",
    "    fin[type_name]='INT'\n",
    "    Big_scorecard=Big_scorecard.append(fin,ignore_index=True, sort=False)\n",
    "    \n",
    "for feature in varsc:\n",
    "    fin=pd.DataFrame()\n",
    "    sss=pd.DataFrame()\n",
    "    ddd=pd.DataFrame()\n",
    "    sss=pd.DataFrame(train_grp.groupby(feature).agg({target_name:['sum','count']}))\n",
    "    sss=pd.DataFrame(sss[target_name])\n",
    "    sss=sss.reset_index()\n",
    "    sss=sss.rename(columns={\"sum\": event_name, \"count\": all_name, feature:grp_name})\n",
    "    sss[nonevent_name]=sss[all_name]-sss[event_name]\n",
    "    sss[event_rate_name]=sss[event_name]/sss[all_name]\n",
    "    sss[logit_name]=np.log((sss[event_name]+0.0001) / (sss[nonevent_name]+0.0001))\n",
    "    sss[share_name]=sss[all_name]/sum\n",
    "    sss[variable_name]=feature    \n",
    "    ddd=pd.DataFrame({grp_name:range(len(feature_intervalsc[feature])),condition_name:feature_intervalsc[feature]})\n",
    "    fin=pd.merge(ddd, sss, on=grp_name)\n",
    "    fin=fin[[variable_name,condition_name,event_rate_name,share_name,all_name,event_name,nonevent_name,logit_name,grp_name]]\n",
    "    fin=fin.sort_values(by=[event_rate_name],ascending=category_order)\n",
    "    fin=fin.reset_index(drop=True)\n",
    "    fin[grp_name]=fin.index\n",
    "    fin[type_name]='NOM'\n",
    "    Big_scorecard=Big_scorecard.append(fin,ignore_index=True, sort=False)\n",
    "\n",
    "Big_scorecard[bad_share]=Big_scorecard[event_name]/sum_bad\n",
    "Big_scorecard[good_share]=Big_scorecard[nonevent_name]/sum_good\n",
    "Big_scorecard[INV]=(Big_scorecard[good_share]-Big_scorecard[bad_share])*np.log((Big_scorecard[good_share]+0.0001) / (Big_scorecard[bad_share]+0.0001))\n",
    "Big_scorecard.to_excel('Big_scorecard.xlsx', index=False)    \n",
    "Big_scorecard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating grp_train and grp_test\n",
    "#feature='app_income'\n",
    "nnn=0\n",
    "grp_train=train.copy() \n",
    "grp_test=test.copy()\n",
    "#grp_train=train_grp.copy()\n",
    "#grp_test=test.copy()\n",
    "for feature in varsn:\n",
    "    sub=pd.DataFrame()\n",
    "    sub=Big_scorecard[Big_scorecard[variable_name]==feature]\n",
    "    sub=sub.reset_index()\n",
    "    nnn+=1\n",
    "    print(nnn,feature)\n",
    "    def grp(x):\n",
    "        res=sub[grp_name][0]            \n",
    "        for i in range(sub.shape[0]):\n",
    "            sl=-np.inf\n",
    "            sr=np.inf\n",
    "            fl=sub[condition_name][i].find(' <= ')\n",
    "            fr=sub[condition_name][i].find(' < ')\n",
    "            if fl>=0:\n",
    "                sl=float(sub[condition_name][i][0:fl])\n",
    "            if fr>=0:\n",
    "                sr=float(sub[condition_name][i][fr+3:])\n",
    "            #fnm=condition_name][i].find(' <> '+symbol_missing)\n",
    "            fm=sub[condition_name][i].find(' = '+symbol_missing)\n",
    "            if fm>=0 and math.isnan(x):                \n",
    "                    res=sub[grp_name][i]            \n",
    "            if fm<0 and (sl <= x < sr):                \n",
    "                    res=sub[grp_name][i] \n",
    "        return res\n",
    "    grp_train[feature]=grp_train[feature].apply(grp)\n",
    "    grp_test[feature]=grp_test[feature].apply(grp)\n",
    "\n",
    "for feature in varsc:\n",
    "    sub=pd.DataFrame()\n",
    "    sub=Big_scorecard[Big_scorecard[variable_name]==feature]\n",
    "    sub=sub.reset_index()\n",
    "    nnn+=1\n",
    "    print(nnn,feature)\n",
    "    def grp(x):\n",
    "        res=sub[grp_name][0] \n",
    "        if type(x) != str:\n",
    "            x=str('')\n",
    "        for i in range(sub.shape[0]):\n",
    "            fo=sub[condition_name][i].find(symbol_other)\n",
    "            if fo>=0:                \n",
    "                    res=sub[grp_name][i]            \n",
    "        for i in range(sub.shape[0]):\n",
    "            fo=sub[condition_name][i].find(symbol_other)\n",
    "            if fo<0 and sub[condition_name][i].find(x)>=0:                \n",
    "                    res=sub[grp_name][i] \n",
    "        return res\n",
    "    grp_train[feature]=grp_train[feature].apply(grp)\n",
    "    grp_test[feature]=grp_test[feature].apply(grp)\n",
    "    \n",
    "    \n",
    "grp_train.head() #moze to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Big_scorecard test\n",
    "Big_scorecard_test=pd.DataFrame()\n",
    "sum=grp_test.shape[0]\n",
    "sum_bad=grp_test[target_name].sum()\n",
    "\n",
    "for feature in vars:\n",
    "    sss=pd.DataFrame()\n",
    "    sss=pd.DataFrame(grp_test.groupby(feature).agg({target_name:['sum','count']}))\n",
    "    sss=pd.DataFrame(sss[target_name])\n",
    "    sss=sss.reset_index()\n",
    "    sss=sss.rename(columns={\"sum\": event_name, \"count\": all_name, feature:grp_name})\n",
    "    sss[nonevent_name]=sss[all_name]-sss[event_name]\n",
    "    sss[share_name_test]=sss[all_name]/sum\n",
    "    sss[bad_share_test]=sss[event_name]/sum_bad\n",
    "    sss[variable_name]=feature\n",
    "    sss=sss[[variable_name,grp_name,share_name_test,bad_share_test]]\n",
    "    Big_scorecard_test=Big_scorecard_test.append(sss,ignore_index=True, sort=False)\n",
    "    \n",
    "\n",
    " \n",
    "Big_scorecard_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating PSI INV\n",
    "Big_scorecard=pd.merge(Big_scorecard, Big_scorecard_test, on=[variable_name,grp_name], how='left')\n",
    "Big_scorecard[PSI]=(Big_scorecard[share_name]-Big_scorecard[share_name_test])*np.log((Big_scorecard[share_name]+0.0001) / (Big_scorecard[share_name_test]+0.0001))\n",
    "Big_scorecard[PSI_tar]=(Big_scorecard[bad_share]-Big_scorecard[bad_share_test])*np.log((Big_scorecard[bad_share]+0.0001) / (Big_scorecard[bad_share_test]+0.0001))\n",
    "\n",
    "Big_scorecard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zmiana\n",
    "def grp(x):\n",
    "    res=sub[logit_name][0]\n",
    "    for i in range(sub.shape[0]):\n",
    "        sl=-np.inf\n",
    "        sr=np.inf\n",
    "        fl=sub[condition_name][i].find(' <= ')\n",
    "        fr=sub[condition_name][i].find(' < ')\n",
    "        if fl>=0:\n",
    "            sl=float(sub[condition_name][i][0:fl])\n",
    "        if fr>=0:\n",
    "            sr=float(sub[condition_name][i][fr+3:])\n",
    "            #fnm=condition_name][i].find(' <> '+symbol_missing)\n",
    "            fm=sub[condition_name][i].find(' = '+symbol_missing)\n",
    "        if fm>=0 and math.isnan(x):     \n",
    "            res=sub[logit_name][i]            \n",
    "        if fm<0 and (sl <= x < sr):\n",
    "            res=sub[logit_name][i] \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating logit_train and logit_test\n",
    "#feature='app_income'\n",
    "nnn=0\n",
    "logit_train=train.copy()\n",
    "logit_test=test.copy()\n",
    "for feature in varsn:\n",
    "    sub=pd.DataFrame()\n",
    "    sub=Big_scorecard[Big_scorecard[variable_name]==feature]\n",
    "    sub=sub.reset_index()\n",
    "    nnn+=1\n",
    "    print(nnn,feature)\n",
    "#     def grp(x):\n",
    "#         res=sub[logit_name][0]            \n",
    "#         for i in range(sub.shape[0]):\n",
    "#             sl=-np.inf\n",
    "#             sr=np.inf\n",
    "#             fl=sub[condition_name][i].find(' <= ')\n",
    "#             fr=sub[condition_name][i].find(' < ')\n",
    "#             if fl>=0:\n",
    "#                 sl=float(sub[condition_name][i][0:fl])\n",
    "#             if fr>=0:\n",
    "#                 sr=float(sub[condition_name][i][fr+3:])\n",
    "#             #fnm=condition_name][i].find(' <> '+symbol_missing)\n",
    "#             fm=sub[condition_name][i].find(' = '+symbol_missing)\n",
    "#             if fm>=0 and math.isnan(x):                \n",
    "#                     res=sub[logit_name][i]            \n",
    "#             if fm<0 and (sl <= x < sr):                \n",
    "#                     res=sub[logit_name][i] \n",
    "#         return res\n",
    "    logit_train[feature]=logit_train[feature].apply(grp)\n",
    "    logit_test[feature]=logit_test[feature].apply(grp)\n",
    "\n",
    "for feature in varsc:\n",
    "    sub=pd.DataFrame()\n",
    "    sub=Big_scorecard[Big_scorecard[variable_name]==feature]\n",
    "    sub=sub.reset_index()\n",
    "    nnn+=1\n",
    "    print(nnn,feature)\n",
    "    def grp(x):\n",
    "        res=sub[logit_name][0] \n",
    "        if type(x) != str:\n",
    "            x=str('')\n",
    "        for i in range(sub.shape[0]):\n",
    "            fo=sub[condition_name][i].find(symbol_other)\n",
    "            if fo>=0:                \n",
    "                    res=sub[logit_name][i]            \n",
    "        for i in range(sub.shape[0]):\n",
    "            fo=sub[condition_name][i].find(symbol_other)\n",
    "            if fo<0 and sub[condition_name][i].find(x)>=0:                \n",
    "                    res=sub[logit_name][i] \n",
    "        return res\n",
    "    logit_train[feature]=logit_train[feature].apply(grp)\n",
    "    logit_test[feature]=logit_test[feature].apply(grp)\n",
    "    \n",
    "    \n",
    "logit_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Gini values for features\n",
    "#feature='app_const'\n",
    "Gini_vars=pd.DataFrame()\n",
    "from sklearn import metrics\n",
    "for feature in vars:\n",
    "    sss=pd.DataFrame([feature], columns=[variable_name])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(logit_train[target_name], logit_train[feature])\n",
    "    gini=np.absolute(2*metrics.auc(fpr, tpr)-1)\n",
    "    sss[gini_train]=gini\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(logit_test[target_name], logit_test[feature])\n",
    "    gini=np.absolute(2*metrics.auc(fpr, tpr)-1)\n",
    "    sss[gini_test]=gini\n",
    "    Gini_vars=Gini_vars.append(sss,ignore_index=True, sort=False)\n",
    "    \n",
    "Gini_vars[delta_gini]=np.absolute(Gini_vars[gini_train]-Gini_vars[gini_test])/Gini_vars[gini_train]\n",
    "Gini_vars[delta_gini]=Gini_vars[delta_gini].fillna(0)\n",
    "Gini_vars=Gini_vars.sort_values(by=[gini_train],ascending=False)\n",
    "Gini_vars=Gini_vars.reset_index(drop=True)\n",
    "\n",
    "sss=pd.DataFrame(Big_scorecard.groupby(variable_name).agg({INV:['sum'],PSI:['sum'],PSI_tar:['sum']}))\n",
    "sss.columns=[INV,PSI,PSI_tar]\n",
    "sss=sss.reset_index()\n",
    "\n",
    "pm = train[vars].isnull().sum() / len(train)\n",
    "stat = pd.DataFrame({'percent_missing': pm}).reset_index()\n",
    "stat.columns=[variable_name,percent_missing]\n",
    "stat[count_unique]=0\n",
    "for i in range(stat.shape[0]):\n",
    "    stat[count_unique][i]=len(pd.unique(train[stat[variable_name][i]]))\n",
    "\n",
    "Gini_vars=pd.merge(Gini_vars, sss, on=variable_name)\n",
    "Gini_vars=pd.merge(Gini_vars, stat, on=variable_name)\n",
    "\n",
    "fin=pd.DataFrame()\n",
    "# feature='ags6_Iqr_Cncr'\n",
    "# feature='app_char_job_code'\n",
    "for feature in vars:\n",
    "    row=pd.DataFrame(np.array([[1,2,3,4]]),columns=[variable_name,mode_name,mode_pname,type_name])\n",
    "    row[variable_name]=feature\n",
    "\n",
    "    ttt=train[[feature]].copy()\n",
    "    ttt=ttt[ttt[feature].isnull()==False]\n",
    "    pm2=ttt[feature].mode()\n",
    "    row[mode_name]=pm2[0]\n",
    "\n",
    "    ttt2=pd.DataFrame(ttt.groupby(feature)[feature].count()/ttt.shape[0])\n",
    "    ttt2.columns=['count']\n",
    "    ttt2=ttt2.reset_index()\n",
    "    ttt2.columns=['var','count']\n",
    "    ttt2=ttt2[ttt2['var']==pm2[0]]\n",
    "    ttt2=ttt2.reset_index()\n",
    "    row[mode_pname]=ttt2['count'][0]\n",
    "    row[type_name]='INT'\n",
    "    for f2 in varsc: \n",
    "        if f2==feature: row[type_name]='NOM'\n",
    "\n",
    "    fin=fin.append(row, ignore_index=True, sort=False)\n",
    "\n",
    "Gini_vars=pd.merge(Gini_vars, fin, on=variable_name)\n",
    "\n",
    "Gini_vars.to_excel('Gini_vars.xlsx', index=False) \n",
    "Gini_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=Gini_vars[\n",
    "    (Gini_vars[gini_train]>0.05) \n",
    "    & (Gini_vars[delta_gini]<0.2)\n",
    "    & (Gini_vars[PSI_tar]<0.1)\n",
    "    & (Gini_vars[PSI]<0.1)\n",
    "].copy()\n",
    "vars_selected=list(sub[variable_name])\n",
    "# vars_selected.remove('act_state_7_CMax_Due')\n",
    "# vars_selected.remove('ags6_Iqr_Cncr')\n",
    "# vars_selected.remove('ags9_Min_CMin_Days')\n",
    "\n",
    "\n",
    "\n",
    "print(len(list(vars_selected)))\n",
    "vars_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating full data\n",
    "grp_all=grp_train.append(grp_test, ignore_index=True, sort=False)\n",
    "print(grp_all.shape, train.shape, test.shape)\n",
    "\n",
    "#grp_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating time variable\n",
    "grp_all[time_report_name]=grp_all[time_name].astype(str).str[0:4]\n",
    "grp_all[[time_name,time_report_name]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable_report\n",
    "startrow=2\n",
    "\n",
    "# Dump Pandas DataFrame to Excel sheet\n",
    "writer = pd.ExcelWriter('Variable_report.xlsx', engine='xlsxwriter')\n",
    "Gini_vars.to_excel(writer, sheet_name=variable_name, startrow=1, index_label=None, index=False)\n",
    "\n",
    "series = Gini_vars[variable_name]\n",
    "max_len = max((series.astype(str).map(len).max(),len(str(series.name)))) + 1 \n",
    "\n",
    "book = writer.book\n",
    "sheet = writer.sheets[variable_name]\n",
    "#format1 = workbook.add_format({'num_format': '#,##0.00'})\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "sheet.set_column('B:B', None, format1)\n",
    "sheet.set_column('C:C', None, format1)\n",
    "sheet.set_column('D:D', None, format1)\n",
    "sheet.set_column(0,0,max_len)\n",
    "sheet.set_column(1,1,len(gini_train))\n",
    "sheet.set_column(2,2,len(gini_test))\n",
    "sheet.set_column(3,3,len(delta_gini))\n",
    "sheet.set_column(4,4,len(INV))\n",
    "sheet.set_column(5,5,len(PSI))\n",
    "sheet.set_column(6,6,len(PSI_tar))\n",
    "sheet.set_column(7,7,len(percent_missing))\n",
    "sheet.set_column(8,8,len(count_unique))\n",
    "\n",
    "sss2=pd.DataFrame(grp_all[time_report_name].value_counts())\n",
    "ntimes=sss2.shape[0]\n",
    "    \n",
    "    \n",
    "for feature in vars_selected:\n",
    "    \n",
    "    sssgr=pd.DataFrame(grp_all.groupby([time_report_name]).agg({target_name:['count']}))\n",
    "    sssgr=pd.DataFrame(sssgr[target_name])\n",
    "    sssgr=sssgr.reset_index()\n",
    "    fin=pd.DataFrame()\n",
    "    sss=pd.DataFrame()\n",
    "    ddd=pd.DataFrame()\n",
    "    sss=pd.DataFrame(grp_all.groupby([feature,time_report_name]).agg({target_name:['sum','count']}))\n",
    "    sss=pd.DataFrame(sss[target_name])\n",
    "    sss=sss.reset_index()\n",
    "    sss=sss.rename(columns={\"sum\": event_name, \"count\": all_name, feature:grp_name})\n",
    "    sss=pd.merge(sss, sssgr, on=time_report_name)\n",
    "    sss[nonevent_name]=sss[all_name]-sss[event_name]\n",
    "    sss[event_rate_name]=sss[event_name]/sss[all_name]\n",
    "    sss[share_name]=sss[all_name]/sss[\"count\"]\n",
    "    sss=sss[[grp_name,time_report_name,event_name,all_name,nonevent_name,event_rate_name,share_name]]\n",
    "    sss=sss.sort_values(by=[grp_name,time_report_name])\n",
    "    \n",
    "    sub=Big_scorecard[Big_scorecard[variable_name]==feature]\n",
    "    sub=sub[[grp_name,condition_name,event_rate_name,share_name,all_name,event_name,nonevent_name]]\n",
    "    ncat=sub.shape[0]\n",
    "    \n",
    "    shname=feature[0:31]\n",
    "    sub.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False)\n",
    "    sss.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False, startcol=8)\n",
    "    book = writer.book\n",
    "    sheet = writer.sheets[shname]\n",
    "    bold = book.add_format({'bold': True, 'size': 24})\n",
    "    sheet.write('A1', variable_name+': '+feature, bold)\n",
    "    bold = book.add_format({'bold': True, 'size': 12})\n",
    "    boldp = book.add_format({'bold': True, 'size': 12, 'num_format': '0.0%'})\n",
    "    sheet.write_formula('D'+str(startrow+ncat+2), '=SUM(D4:D'+str(startrow+ncat+1)+')', boldp)\n",
    "    sheet.write_formula('E'+str(startrow+ncat+2), '=SUM(E4:E'+str(startrow+ncat+1)+')', bold)\n",
    "    sheet.write_formula('F'+str(startrow+ncat+2), '=SUM(F4:F'+str(startrow+ncat+1)+')', bold)\n",
    "    sheet.write_formula('G'+str(startrow+ncat+2), '=SUM(G4:G'+str(startrow+ncat+1)+')', bold)\n",
    "\n",
    "    #format1 = workbook.add_format({'num_format': '#,##0.00'})\n",
    "    format1 = book.add_format({'num_format': '0.0%'})\n",
    "    sheet.set_column('C:C', None, format1)\n",
    "    sheet.set_column('D:D', None, format1)\n",
    "    sheet.set_column('N:N', None, format1)\n",
    "    sheet.set_column('O:O', None, format1)\n",
    "\n",
    "    series = sub[condition_name]\n",
    "    max_len = max((series.astype(str).map(len).max(),len(str(series.name)))) + 1\n",
    "    sheet.set_column(1,1,max_len)\n",
    "\n",
    "    # Chart\n",
    "    chart = book.add_chart({'type': 'line'})\n",
    "    chart.set_title({'name': event_rate_name})\n",
    "    chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "    for i in range(ncat):\n",
    "        chart.add_series({'values': '='+shname+'!N'+str(4+i*ntimes)+':N'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                         'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "    chart.set_legend({'position': 'bottom'})\n",
    "    sheet.insert_chart('A'+str(ncat+6), chart)\n",
    "\n",
    "\n",
    "    chart = book.add_chart({'type': 'line'})\n",
    "    chart.set_title({'name': share_name})\n",
    "    chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "    for i in range(ncat):\n",
    "        chart.add_series({'values': '='+shname+'!O'+str(4+i*ntimes)+':O'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                         'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "    chart.set_legend({'position': 'bottom'})\n",
    "    sheet.insert_chart('A'+str(ncat+6+18), chart)\n",
    "#zmiana\n",
    "#writer.save()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple RFE selection method\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Logit \n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#model = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000) #zamiana\n",
    "model = LogisticRegression(solver='liblinear', multi_class='auto',max_iter=1000) #zmiana\n",
    "rfe = RFE(estimator=model, n_features_to_select=1, step=1)\n",
    "rfe.fit(logit_train[vars_selected], logit_train[target_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessment of combinations of features\n",
    "description_results += \"number_vars=30 number_features=7\"\n",
    "\n",
    "number_vars=30\n",
    "number_features=7\n",
    "\n",
    "#number_vars=10\n",
    "#number_features=5\n",
    "\n",
    "#number_vars=15\n",
    "#number_features=6\n",
    "\n",
    "resultr = list(rfe.ranking_ <= number_vars)\n",
    "selected_features = [vars_selected[i] for i, val in enumerate(resultr) if val == 1]\n",
    "\n",
    "Model_list=pd.DataFrame()\n",
    "\n",
    "def assess(selected_vars):\n",
    "    Model_list0=pd.DataFrame(np.array([['b',2,'b',1.99,1.99,1.99,1.99,1.99,1.99]]),\n",
    "        columns=['Variables','nnegative_betas','max_pvalue','gini_train','gini_test','delta_gini','max_vif','max_con_index','max_pearson',])\n",
    "\n",
    "    var_list=''\n",
    "    for i,v in enumerate(selected_vars):\n",
    "        if i==0:\n",
    "            var_list=v\n",
    "        else:\n",
    "            var_list=var_list+','+v\n",
    "\n",
    "    Model_list0['Variables'][0]=var_list\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    from sklearn.metrics import auc, roc_curve \n",
    "    from statsmodels.discrete.discrete_model import Logit \n",
    "\n",
    "    features = selected_vars+[intercept_name]\n",
    "    X=logit_train[features]\n",
    "    y=logit_train[target_name]\n",
    "    X_test=logit_test[features]\n",
    "    y_test=logit_test[target_name]\n",
    "\n",
    "    model = sm.Logit(y, X).fit(disp = 0,method='newton')\n",
    "\n",
    "    pv=0\n",
    "    nnegative_betas=0\n",
    "    for i in range(len(list(model.params))):\n",
    "        if model.params[i]<0 and model.params.index[i]!=intercept_name: \n",
    "            nnegative_betas+=1\n",
    "        if model.params.index[i]!=intercept_name: \n",
    "            pv=max(pv,model.pvalues[i])\n",
    "    max_pvalue=pv\n",
    "\n",
    "    pre = model.predict(X)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pre)\n",
    "    gini_train = np.absolute(2 * auc(fpr, tpr) - 1)\n",
    "\n",
    "    pre = model.predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pre)\n",
    "    gini_test = np.absolute(2 * auc(fpr, tpr) - 1)\n",
    "\n",
    "    delta_gini=np.absolute(gini_train-gini_test)/gini_train\n",
    "    if math.isnan(delta_gini): delta_gini=0\n",
    "\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vif=1\n",
    "    for i in range(X.shape[1]):\n",
    "        if list(X)[i]!=intercept_name:\n",
    "            vif = max(float(variance_inflation_factor(np.asarray(X), i)),vif)\n",
    "    max_vif=vif\n",
    "\n",
    "    X_new = X/(((X*X).sum())**0.5)\n",
    "    Xt = np.transpose(X_new)\n",
    "    XtX = np.dot(Xt,X_new)\n",
    "    Eig = np.linalg.eig(XtX)[0]\n",
    "    max_con_index = np.sqrt(np.max(Eig) / np.min(Eig))\n",
    "\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    upper = upper.fillna(0)\n",
    "    max_pearson=max(upper.max())\n",
    "\n",
    "    Model_list0['nnegative_betas'][0]=int(nnegative_betas)\n",
    "    Model_list0['max_pvalue'][0]=float(max_pvalue)\n",
    "    Model_list0['gini_train'][0]=float(gini_train)\n",
    "    Model_list0['gini_test'][0]=float(gini_test)\n",
    "    Model_list0['delta_gini'][0]=float(delta_gini)\n",
    "    Model_list0['max_vif'][0]=float(max_vif)\n",
    "    Model_list0['max_con_index'][0]=float(max_con_index)\n",
    "    Model_list0['max_pearson'][0]=float(max_pearson)\n",
    "\n",
    "    return Model_list0\n",
    "\n",
    "#Model_list=Model_list.append(assess(selected_features), ignore_index=True, sort=False)\n",
    "#zmiana\n",
    "Model_list = pd.concat([Model_list, assess(selected_features)], ignore_index=True, sort=False)\n",
    "\n",
    "index=2**number_vars-1\n",
    "for i in range(index):\n",
    "    get_bin = lambda i, n: format(i, 'b').zfill(n)\n",
    "    bin=get_bin(i,number_vars)\n",
    "    suma=0\n",
    "    selected_v=list([])\n",
    "    for i,p in enumerate(bin):\n",
    "        if p=='1': \n",
    "            suma=suma+1\n",
    "            selected_v.append(selected_features[i])\n",
    "    if suma==number_features:\n",
    "#         print(selected_v)\n",
    "        #Model_list=Model_list.append(assess(selected_v), ignore_index=True, sort=False)\n",
    "    #zmiana - FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version\n",
    "    #To resolve this issue, will use the pandas.concat() function instead of .append() method \n",
    "        Model_list = pd.concat([Model_list, assess(selected_v)], ignore_index=True, sort=False)\n",
    "\n",
    "Model_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subModel_list=Model_list[\n",
    "    (Model_list['nnegative_betas']==0) \n",
    "    & (Model_list['max_pvalue']<=0.01) \n",
    "    & (Model_list['max_vif']<=3.0) \n",
    "].copy()\n",
    "\n",
    "subModel_list=subModel_list.sort_values(by=['gini_test'],ascending=False)\n",
    "subModel_list=subModel_list.reset_index()\n",
    "subModel_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subModel_list['Variables'][0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model assessment based on selected variables selected_features\n",
    "\n",
    "selected_features=subModel_list['Variables'][0].split(',')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import auc, roc_curve \n",
    "from statsmodels.discrete.discrete_model import Logit \n",
    "\n",
    "features = selected_features+[intercept_name]\n",
    "X=logit_train[features]\n",
    "y=logit_train[target_name]\n",
    "X_test=logit_test[features]\n",
    "y_test=logit_test[target_name]\n",
    "\n",
    "result = {'Efects': pd.DataFrame(),\n",
    "          gini_train: np.nan,\n",
    "          gini_test: np.nan,\n",
    "          delta_gini: np.nan,\n",
    "          max_pvalue: '',\n",
    "          nnegative_betas: 0,\n",
    "          max_vif: np.nan,\n",
    "          max_pearson: np.nan,\n",
    "          max_con_index: np.nan,\n",
    "          'KS score': np.nan,\n",
    "          'PSI score': np.nan,\n",
    "          'Gains1': np.nan,\n",
    "          'Gains2': np.nan,\n",
    "          'Gains3': np.nan,\n",
    "          'Gains4': np.nan,\n",
    "          'Gains5': np.nan,\n",
    "          'Gains10': np.nan,\n",
    "          'Gains50': np.nan,\n",
    "          'Lift1': np.nan,\n",
    "          'Lift2': np.nan,\n",
    "          'Lift3': np.nan,\n",
    "          'Lift4': np.nan,\n",
    "          'Lift5': np.nan,\n",
    "          'Lift10': np.nan,\n",
    "          'Lift50': np.nan\n",
    "          }\n",
    "\n",
    "model = sm.Logit(y, X).fit(disp = 0,method='newton')\n",
    "\n",
    "#result['Coef']=model.params\n",
    "#result['P_value']=model.pvalues\n",
    "ggg=model.wald_test_terms().summary_frame().reset_index()\n",
    "ggg.columns=[variable_name,wald_test,pvalue,degree_free]\n",
    "ppp=pd.DataFrame(model.params,columns=[estimation]).reset_index()\n",
    "ppp.columns=[variable_name,estimation]\n",
    "ppp=pd.merge(ppp, ggg, on=variable_name)\n",
    "se=model.summary2().tables[1].reset_index()\n",
    "se.columns=[variable_name,'c',std_err,'z','p','l','r']\n",
    "se=se[[variable_name,std_err]]\n",
    "ppp=pd.merge(ppp, se, on=variable_name)\n",
    "ppp=ppp[[variable_name,degree_free,estimation,std_err,wald_test,pvalue]]\n",
    "\n",
    "def pval(x):\n",
    "    if x>0.0001: wyn='%.4f' % x\n",
    "    else: wyn='<.0001'\n",
    "    return wyn\n",
    "\n",
    "ppp[pvalue]=ppp[pvalue].apply(pval)\n",
    "\n",
    "result['Efects']=ppp\n",
    "\n",
    "pv=0\n",
    "for i in range(len(list(model.params))):\n",
    "    if model.params[i]<0 and model.params.index[i]!=intercept_name: \n",
    "        result[nnegative_betas]+=1\n",
    "    if model.params.index[i]!=intercept_name: \n",
    "        pv=max(pv,model.pvalues[i])\n",
    "#result[max_pvalue]=float(round(pv, 6))\n",
    "result[max_pvalue]=pval(pv)\n",
    "\n",
    "pre = model.predict(X)\n",
    "fpr, tpr, thresholds = roc_curve(y, pre)\n",
    "result[gini_train] = float(np.absolute(2 * auc(fpr, tpr) - 1))\n",
    "\n",
    "pre = model.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pre)\n",
    "result[gini_test] = float(np.absolute(2 * auc(fpr, tpr) - 1))\n",
    "\n",
    "result[delta_gini]=np.absolute(result[gini_train]-result[gini_test])/result[gini_train]\n",
    "if math.isnan(result[delta_gini]): result[delta_gini]=0.0\n",
    "    \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif=1\n",
    "for i in range(X.shape[1]):\n",
    "    if list(X)[i]!=intercept_name:\n",
    "        vif = max(float(variance_inflation_factor(np.asarray(X), i)),vif)\n",
    "result[max_vif]=float(vif)\n",
    "\n",
    "X_new = X/(((X*X).sum())**0.5)\n",
    "Xt = np.transpose(X_new)\n",
    "XtX = np.dot(Xt,X_new)\n",
    "Eig = np.linalg.eig(XtX)[0]\n",
    "result[max_con_index] = float(np.sqrt(np.max(Eig) / np.min(Eig)))\n",
    "\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "upper = upper.fillna(0)\n",
    "result[max_pearson]=float(max(upper.max()))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Scorecard\n",
    "features=list(result['Efects'][variable_name].copy())\n",
    "features.remove(intercept_name)\n",
    "Scorecard=Big_scorecard[Big_scorecard[variable_name].isin(features)].copy()\n",
    "coef=result['Efects']\n",
    "coef=coef[[variable_name,estimation]]\n",
    "coef.columns=[variable_name,beta_name]\n",
    "alpha=float(coef[coef[variable_name]==intercept_name][beta_name])\n",
    "Scorecard=pd.merge(Scorecard, coef, on=variable_name)\n",
    "factor = 20/np.log(2)\n",
    "v = len(features)\n",
    "alpp=Scorecard[logit_name]*Scorecard[beta_name]*factor\n",
    "alp=-alpp.sum()+300\n",
    "alpp2=Scorecard.copy()\n",
    "alpp2=alpp2[alpp2[grp_name]==0]\n",
    "alpp2[fbeta_name]=alpp2[logit_name]*alpp2[beta_name]\n",
    "alpp2=alpp2[[variable_name,fbeta_name]]\n",
    "Scorecard=pd.merge(Scorecard, alpp2, on=variable_name)\n",
    "Scorecard[score_name]=-(Scorecard[logit_name]*Scorecard[beta_name]-Scorecard[fbeta_name] + alpha/v)*factor+alp/v\n",
    "Scorecard[score_name]=round(Scorecard[score_name])\n",
    "#Scorecard.to_excel('Scorecard.xlsx', index=False) \n",
    "Scorecard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating full scored data\n",
    "#scored_all=grp_train.append(grp_test, ignore_index=True, sort=False)\n",
    "scored_all=grp_all.copy()\n",
    "features=list(result['Efects'][variable_name].copy())\n",
    "features.remove(intercept_name)\n",
    "scored_all=scored_all[features+[target_name]+[time_name]+[time_report_name]+[event_value]+[all_value]+[id_row]]\n",
    "\n",
    "for feature in features:\n",
    "    scored_all=scored_all.rename(columns={feature: grp_name})\n",
    "    sc=Scorecard[Scorecard[variable_name]==feature].copy()\n",
    "    sc=sc[[grp_name,score_name]]\n",
    "    scored_all=pd.merge(scored_all, sc, on=grp_name)\n",
    "    scored_all=scored_all.rename(columns={score_name: feature})\n",
    "    scored_all=scored_all.drop(grp_name,1)\n",
    "scored_all[score_name]=0\n",
    "for feature in features:\n",
    "    scored_all[score_name]=scored_all[score_name]+scored_all[feature]\n",
    "#scored_all.to_excel('Scored_all.xlsx', index=False) \n",
    "#scored_all.shape\n",
    "scored_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train scored data\n",
    "scored_train=grp_train.copy()\n",
    "features=list(result['Efects'][variable_name].copy())\n",
    "features.remove(intercept_name)\n",
    "scored_train=scored_train[features+[target_name]+[event_value]+[all_value]+[id_row]]\n",
    "\n",
    "for feature in features:\n",
    "    scored_train=scored_train.rename(columns={feature: grp_name})\n",
    "    sc=Scorecard[Scorecard[variable_name]==feature].copy()\n",
    "    sc=sc[[grp_name,score_name]]\n",
    "    scored_train=pd.merge(scored_train, sc, on=grp_name)\n",
    "    scored_train=scored_train.rename(columns={score_name: feature})\n",
    "    scored_train=scored_train.drop(grp_name,1)\n",
    "scored_train[score_name]=0\n",
    "for feature in features:\n",
    "    scored_train[score_name]=scored_train[score_name]+scored_train[feature]\n",
    "\n",
    "# scored_train.head()\n",
    "# scored_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test scored data\n",
    "scored_test=grp_test.copy()\n",
    "features=list(result['Efects'][variable_name].copy())\n",
    "features.remove(intercept_name)\n",
    "scored_test=scored_test[features+[target_name]+[event_value]+[all_value]+[id_row]]\n",
    "\n",
    "for feature in features:\n",
    "    scored_test=scored_test.rename(columns={feature: grp_name})\n",
    "    sc=Scorecard[Scorecard[variable_name]==feature].copy()\n",
    "    sc=sc[[grp_name,score_name]]\n",
    "    scored_test=pd.merge(scored_test, sc, on=grp_name)\n",
    "    scored_test=scored_test.rename(columns={score_name: feature})\n",
    "    scored_test=scored_test.drop(grp_name,1)\n",
    "scored_test[score_name]=0\n",
    "for feature in features:\n",
    "    scored_test[score_name]=scored_test[score_name]+scored_test[feature]\n",
    "\n",
    "# scored_test.head()\n",
    "# scored_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks(data=None,target=None, prob=None):\n",
    "    data['target0'] = 1 - data[target]\n",
    "#     ncat=10\n",
    "#     data['bucket'] = pd.qcut(data[prob], ncat)\n",
    "#     grouped = data.groupby('bucket', as_index = False)\n",
    "    grouped = data.groupby(prob, as_index = False)\n",
    "    kstable = pd.DataFrame()\n",
    "    kstable['min_prob'] = grouped.min()[prob]\n",
    "    kstable['max_prob'] = grouped.max()[prob]\n",
    "    kstable['events']   = grouped.sum()[target]\n",
    "    kstable['allq']   = grouped.count()[target]\n",
    "    ssss=kstable['allq'].sum()\n",
    "    kstable['nonevents'] = grouped.sum()['target0']\n",
    "    kstable = kstable.sort_values(by=\"min_prob\", ascending=True).reset_index(drop = True)\n",
    "#     kstable['event_rate'] = (kstable.events / data[target].sum()).apply('{0:.2%}'.format)\n",
    "#     kstable['nonevent_rate'] = (kstable.nonevents / data['target0'].sum()).apply('{0:.2%}'.format)\n",
    "    kstable['cum_eventrate']=(kstable.events / data[target].sum()).cumsum()\n",
    "    kstable['cum_noneventrate']=(kstable.nonevents / data['target0'].sum()).cumsum()\n",
    "    kstable['cum_all']=(kstable.allq / ssss).cumsum()\n",
    "    kstable['KS'] = np.round(kstable['cum_eventrate']-kstable['cum_noneventrate'], 5)\n",
    "\n",
    "    #Formating\n",
    "#     kstable['cum_eventrate']= kstable['cum_eventrate'].apply('{0:.2%}'.format)\n",
    "#     kstable['cum_noneventrate']= kstable['cum_noneventrate'].apply('{0:.2%}'.format)\n",
    "#     kstable['cum_all']= kstable['cum_all'].apply('{0:.2%}'.format)\n",
    "#     kstable.index = range(1,ncat+1)\n",
    "#     kstable.index.rename('Decile', inplace=True)\n",
    "#     pd.set_option('display.max_columns', 9)\n",
    "    #print(kstable)\n",
    "    \n",
    "    #Display KS\n",
    "#     from colorama import Fore\n",
    "#     print(Fore.RED + \"KS is \" + str(max(kstable['KS']))+\"%\"+ \" at decile \" + str((kstable.index[kstable['KS']==max(kstable['KS'])][0])))\n",
    "    return(kstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating KS score, PSI Score, Lifts and Gains\n",
    "data=scored_test.copy()\n",
    "kst=ks(data=data,target=target_name, prob=score_name)\n",
    "result['KS score']=float(kst['KS'].max())\n",
    "\n",
    "lift_list=[1,2,3,4,5,10,50]\n",
    "for cen in lift_list:\n",
    "    per=cen/100\n",
    "    kst1=kst[kst['cum_all']<=per].copy()\n",
    "    gains=kst1['cum_eventrate'].max()\n",
    "    result['Gains'+str(cen)]=float(gains)\n",
    "    result['Lift'+str(cen)]=float(gains/per)\n",
    "    \n",
    "datate=scored_test.copy()\n",
    "datate['dataset']='Test'\n",
    "datatr=scored_train.copy()\n",
    "datatr['dataset']='Train'\n",
    "data_all=datatr.append(datate, ignore_index=True, sort=False)\n",
    "data_all=data_all[[score_name,'dataset']]\n",
    "# print(datatr.shape, datate.shape, data_all.shape)\n",
    "# data_all.head()\n",
    "\n",
    "data_all['bucket'] = pd.qcut(data_all[score_name], 5)\n",
    "\n",
    "ssstr=pd.DataFrame(data_all[data_all['dataset']=='Train'].groupby('bucket').count())\n",
    "ssstr=ssstr.reset_index()\n",
    "ssstr['tr']=ssstr['dataset']/datatr.shape[0]\n",
    "ssstr=ssstr[['bucket','tr']]\n",
    "\n",
    "ssste=pd.DataFrame(data_all[data_all['dataset']=='Test'].groupby('bucket').count())\n",
    "ssste=ssste.reset_index()\n",
    "ssste['te']=ssste['dataset']/datate.shape[0]\n",
    "ssste=ssste[['bucket','te']]\n",
    "\n",
    "sssall=pd.merge(ssste, ssstr, on=['bucket'], how='outer')\n",
    "sssall['te']=sssall['te'].fillna(0)\n",
    "sssall['tr']=sssall['tr'].fillna(0)\n",
    "\n",
    "sssall[PSI]=(sssall['tr']-sssall['te'])*np.log((sssall['tr']+0.0001) / (sssall['te']+0.0001))\n",
    "\n",
    "result['PSI score']=float(sssall[PSI].sum())\n",
    "    \n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating gini_curves_model\n",
    "sc=scored_all[[score_name,target_name]].copy()\n",
    "sc['rank'] = sc[score_name].rank()\n",
    "sc['grouping'] = round(sc['rank']*(20-1)/(len(sc[score_name])+1))\n",
    "sss=pd.DataFrame(sc.groupby(['grouping']).agg({target_name:['sum','count']}))\n",
    "sss=pd.DataFrame(sss[target_name])\n",
    "sss=sss.reset_index()\n",
    "sss['count']=sss['count']-sss['sum']\n",
    "sss=sss.rename(columns={'count':'goods', 'sum':'bads'})\n",
    "sss=sss[['bads','goods']]\n",
    "import openpyxl\n",
    "wb = openpyxl.load_workbook('ASB_PYTHON/gini_curves_template.xlsx')\n",
    "ws = wb.active\n",
    "for i in range(sss.shape[0]):\n",
    "    ws.cell(row=9+i, column=4).value = sss['bads'][i]\n",
    "    ws.cell(row=9+i, column=5).value = sss['goods'][i]\n",
    "wb.save('ASB_PYTHON/gini_curves_model.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini on scored_all\n",
    "fpr, tpr, thresholds = metrics.roc_curve(scored_all[target_name], scored_all[score_name])\n",
    "gini=np.absolute(2*metrics.auc(fpr, tpr)-1)\n",
    "gini\n",
    "#0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini over time\n",
    "#scored=scored_all[[target_name, score_name, time_report_name]].copy()\n",
    "#zmiana\n",
    "scored=scored_all.loc[:, [target_name, score_name, time_report_name]].copy()\n",
    "ulist=np.unique(scored[time_report_name])\n",
    "time_gini=pd.DataFrame(ulist,columns=[time_report_name])\n",
    "time_gini['Gini']=0.0\n",
    "\n",
    "for t in range(len(ulist)):\n",
    "    #scored_sub=scored[scored[time_report_name]==ulist[t]].copy()\n",
    "    #zmiana\n",
    "    scored_sub=scored.loc[scored[time_report_name] == ulist[t], :].copy()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(scored_sub[target_name], scored_sub[score_name])\n",
    "    #time_gini['Gini'][t]=np.absolute(2*metrics.auc(fpr, tpr)-1)\n",
    "    #zmiana\n",
    "    time_gini.loc[t, 'Gini'] = np.absolute(2*metrics.auc(fpr, tpr)-1)\n",
    "time_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration\n",
    "cal_scored=scored_all[[id_row]+[score_name,target_name,time_report_name]].copy()\n",
    "cal_scored[intercept_name]=1\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "features=[score_name,intercept_name]\n",
    "X=cal_scored[features]\n",
    "y=cal_scored[target_name]\n",
    "\n",
    "model_cal=sm.Logit(y,X).fit(disp=0,method='newton')\n",
    "\n",
    "def kal(x):\n",
    "    r=1/(1+math.exp(-(model_cal.params[0]*x+model_cal.params[1])))\n",
    "    if r<0: r=0\n",
    "    if r>1: r=1\n",
    "    return r\n",
    "\n",
    "cal_scored[prob_event]=cal_scored[score_name].apply(kal)\n",
    "scored_all[prob_event]=scored_all[score_name].apply(kal)\n",
    "print(model_cal.params)\n",
    "print(model_cal.params[0],model_cal.params[1])\n",
    "cal_scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_scored.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalt=pd.DataFrame(cal_scored.groupby([time_report_name]).agg({target_name:['count','mean'], prob_event:['mean']}))\n",
    "kalt=kalt.reset_index()\n",
    "kalt.columns=[time_report_name,'ALL',event_rate_name,prob_event]\n",
    "kalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional calibration test\n",
    "\n",
    "n_groups=10\n",
    "sc=scored_all.copy()\n",
    "sc['rank'] = sc[score_name].rank()\n",
    "sc['grouping'] = round(sc['rank']*(n_groups-1)/(len(sc[score_name])+1))\n",
    "sss=pd.DataFrame(sc.groupby(['grouping']).agg({\n",
    "    score_name:['min','max'],\n",
    "    target_name:['mean'],\n",
    "    'PD': ['mean']\n",
    "}))\n",
    "sss=sss.reset_index()\n",
    "sss.columns=['Segment','Min score','Max score', event_rate_name,'PD']\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(columns[i])+str(columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model report\n",
    "startrow=2\n",
    "\n",
    "features=list(result['Efects'][variable_name].copy())\n",
    "Gini_varsm=Gini_vars[Gini_vars[variable_name].isin(features)].copy()\n",
    "\n",
    "result2=result.copy()\n",
    "del result2['Efects']\n",
    "rrr=pd.DataFrame.from_dict(result2, orient='index')\n",
    "rrr=rrr.reset_index()\n",
    "rrr.columns=['Measure','Value']\n",
    "\n",
    "series = Gini_varsm[variable_name]\n",
    "features=series\n",
    "max_len = max((series.astype(str).map(len).max(),len(str(series.name)))) + 1 \n",
    "\n",
    "\n",
    "# Dump Pandas DataFrame to Excel sheet\n",
    "writer = pd.ExcelWriter('Model_report.xlsx', engine='xlsxwriter')\n",
    "rrr.to_excel(writer, sheet_name='Main_measures', startrow=1, index_label=None, index=False)\n",
    "result['Efects'].to_excel(writer, sheet_name='Effects', startrow=1, index_label=None, index=False)\n",
    "time_gini.to_excel(writer, sheet_name='Gini_over_time', startrow=1, index_label=None, index=False)\n",
    "book = writer.book\n",
    "sheet = writer.sheets['Gini_over_time']\n",
    "bold = book.add_format({'bold': True, 'size': 13})\n",
    "sheet.write('A1', 'Gini all:', bold)\n",
    "boldp = book.add_format({'bold': True, 'size': 13, 'num_format': '0.0%'})\n",
    "sheet.write('B1', str(gini), boldp)\n",
    "bold = book.add_format({'bold': True, 'size': 12})\n",
    "boldp = book.add_format({'bold': True, 'size': 12, 'num_format': '0.0%'})\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "sheet.set_column('B:B', None, format1)\n",
    "\n",
    "ntimes=time_gini.shape[0]\n",
    "shname='Gini_over_time'\n",
    "chart = book.add_chart({'type': 'line'})\n",
    "chart.set_title({'name': 'Gini over time'})\n",
    "chart.set_x_axis({'name': '='+shname+'!A2', 'num_font':  {'rotation': 45}})\n",
    "chart.add_series({'values': '='+shname+'!B3:B'+str(3+ntimes), 'name': '='+shname+'!B2',\n",
    "                         'categories': '='+shname+'!A3:A'+str(3+ntimes)})\n",
    "chart.set_legend({'position': 'bottom'})\n",
    "sheet.insert_chart('D2', chart)\n",
    "\n",
    "Scorecard.to_excel(writer, sheet_name='Scorecard', startrow=1, index_label=None, index=False)\n",
    "\n",
    "scale=pd.DataFrame(Scorecard.groupby([variable_name]).agg({score_name:['min','max']}))\n",
    "scale=scale.reset_index()\n",
    "scale.columns=[variable_name,'Min score','Max score']\n",
    "scale['Range']=scale['Max score']-scale['Min score']\n",
    "ranges=scale['Max score'].sum()-scale['Min score'].sum()\n",
    "scale['Importance']=scale['Range']/ranges\n",
    "scale=scale.sort_values(by=['Importance'],ascending=False)\n",
    "scale.to_excel(writer, sheet_name='Variable importance', startrow=1, index_label=None, index=False)\n",
    "sheet = writer.sheets['Variable importance']\n",
    "sheet.set_column(0,0,max_len)\n",
    "sheet.set_column(1,1,len('Min score'))\n",
    "sheet.set_column(2,2,len('Max score'))\n",
    "sheet.set_column(3,3,len('Range'))\n",
    "sheet.set_column(4,4,len('Variable importance'))\n",
    "\n",
    "kalt.to_excel(writer, sheet_name='Calibration', startrow=0, index_label=None, index=False)\n",
    "\n",
    "book = writer.book\n",
    "sheet = writer.sheets['Calibration']\n",
    "bold = book.add_format({'bold': True, 'size': 13})\n",
    "sheet.write('F1', 'Score coeficient: '+str(model_cal.params[0]), bold)\n",
    "sheet.write('F2', 'Intercept: '+str(model_cal.params[1]), bold)\n",
    "\n",
    "sheet.write('F3', 'Formula: '+prob_event+'=1/(1+exp(-('+\n",
    "            str(model_cal.params[0])+'*'+score_name+'+('\n",
    "            +str(model_cal.params[1])+'))))', bold)\n",
    "\n",
    "\n",
    "sheet.write('F5', event_rate_name+': '+str(cal_scored[target_name].mean()), bold)\n",
    "sheet.write('F6', prob_event+': '+str(cal_scored[prob_event].mean()), bold)\n",
    "\n",
    "\n",
    "\n",
    "Gini_varsm.to_excel(writer, sheet_name=variable_name, startrow=1, index_label=None, index=False)\n",
    "\n",
    "book = writer.book\n",
    "sheet = writer.sheets[variable_name]\n",
    "#format1 = workbook.add_format({'num_format': '#,##0.00'})\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "#zmiana\n",
    "# sheet.set_column('B:B', None, format1)\n",
    "# sheet.set_column('C:C', None, format1)\n",
    "# sheet.set_column('D:D', None, format1)\n",
    "# sheet.set_column(0,0, max_len)\n",
    "# sheet.set_column(1,1,len(gini_train))\n",
    "# sheet.set_column(2,2,len(gini_test))\n",
    "# sheet.set_column(3,3,len(delta_gini))\n",
    "# sheet.set_column(4,4,len(INV))\n",
    "# sheet.set_column(5,5,len(PSI))\n",
    "# sheet.set_column(6,6,len(PSI_tar))\n",
    "# sheet.set_column(7,7,len(percent_missing))\n",
    "# sheet.set_column(8,8,len(count_unique))\n",
    "\n",
    "columns = ['B', 'C', 'D', 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "value = [None, None, None, max_len, len(gini_train), len(gini_test), len(delta_gini), len(INV), len(PSI), len(PSI_tar), len(percent_missing), len(count_unique)]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    if value[i] is None:\n",
    "        sheet.set_column(str(columns[i])+':'+str(columns[i]), None, format1)\n",
    "    else:\n",
    "        sheet.set_column(columns[i], columns[i], value[i])\n",
    "    \n",
    "sss2=pd.DataFrame(grp_all[time_report_name].value_counts())\n",
    "ntimes=sss2.shape[0]\n",
    "    \n",
    "for feature in features:\n",
    "    sssgr=pd.DataFrame(grp_all.groupby([time_report_name]).agg({target_name:['count']}))\n",
    "    sssgr=pd.DataFrame(sssgr[target_name])\n",
    "    sssgr=sssgr.reset_index()\n",
    "    fin=pd.DataFrame()\n",
    "    sss=pd.DataFrame()\n",
    "    ddd=pd.DataFrame()\n",
    "    sss=pd.DataFrame(grp_all.groupby([feature,time_report_name]).agg({target_name:['sum','count']}))\n",
    "    sss=pd.DataFrame(sss[target_name])\n",
    "    sss=sss.reset_index()\n",
    "    sss=sss.rename(columns={\"sum\": event_name, \"count\": all_name, feature:grp_name})\n",
    "    sss=pd.merge(sss, sssgr, on=time_report_name)\n",
    "    sss[nonevent_name]=sss[all_name]-sss[event_name]\n",
    "    sss[event_rate_name]=sss[event_name]/sss[all_name]\n",
    "    sss[share_name]=sss[all_name]/sss[\"count\"]\n",
    "    sss=sss[[grp_name,time_report_name,event_name,all_name,nonevent_name,event_rate_name,share_name]]\n",
    "    sss=sss.sort_values(by=[grp_name,time_report_name])\n",
    "    \n",
    "    sub=Scorecard[Scorecard[variable_name]==feature]\n",
    "    sub2=sub[[grp_name,score_name]]\n",
    "    sub=sub[[score_name,condition_name,event_rate_name,share_name,all_name,event_name,nonevent_name]]\n",
    "    sss=pd.merge(sss, sub2, on=grp_name)\n",
    "    sss=sss[[score_name,time_report_name,event_name,all_name,nonevent_name,event_rate_name,share_name]]\n",
    "    ncat=sub.shape[0]\n",
    "    \n",
    "    shname=feature[0:31]\n",
    "    sub.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False)\n",
    "    sss.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False, startcol=8)\n",
    "    book = writer.book\n",
    "    sheet = writer.sheets[shname]\n",
    "    bold = book.add_format({'bold': True, 'size': 24})\n",
    "    sheet.write('A1', variable_name+': '+feature, bold)\n",
    "    bold = book.add_format({'bold': True, 'size': 12})\n",
    "    boldp = book.add_format({'bold': True, 'size': 12, 'num_format': '0.0%'})\n",
    "    sheet.write_formula('D'+str(startrow+ncat+2), '=SUM(D4:D'+str(startrow+ncat+1)+')', boldp)\n",
    "    sheet.write_formula('E'+str(startrow+ncat+2), '=SUM(E4:E'+str(startrow+ncat+1)+')', bold)\n",
    "    sheet.write_formula('F'+str(startrow+ncat+2), '=SUM(F4:F'+str(startrow+ncat+1)+')', bold)\n",
    "    sheet.write_formula('G'+str(startrow+ncat+2), '=SUM(G4:G'+str(startrow+ncat+1)+')', bold)\n",
    "\n",
    "\n",
    "    format1 = book.add_format({'num_format': '0.0%'})\n",
    "    #zmiana\n",
    "#     sheet.set_column('C:C', None, format1)\n",
    "#     sheet.set_column('D:D', None, format1)\n",
    "#     sheet.set_column('N:N', None, format1)\n",
    "#     sheet.set_column('O:O', None, format1)\n",
    "    \n",
    "    columns = ['C','D','N','O']\n",
    "    value = [None, None, None, None]\n",
    "    for i in range(len(columns)):\n",
    "        sheet.set_column(str(columns[i])+':'+str(columns[i]), None, format1)\n",
    "\n",
    "    series = sub[condition_name]\n",
    "    max_len = max((series.astype(str).map(len).max(),len(str(series.name)))) + 1\n",
    "    sheet.set_column(1,1,max_len)\n",
    "\n",
    "\n",
    "    chart = book.add_chart({'type': 'line'})\n",
    "    chart.set_title({'name': event_rate_name})\n",
    "    chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "    for i in range(ncat):\n",
    "        chart.add_series({'values': '='+shname+'!N'+str(4+i*ntimes)+':N'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                         'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "    chart.set_legend({'position': 'bottom'})\n",
    "    sheet.insert_chart('A'+str(ncat+6), chart)\n",
    "\n",
    "\n",
    "    chart = book.add_chart({'type': 'line'})\n",
    "    chart.set_title({'name': share_name})\n",
    "    chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "    for i in range(ncat):\n",
    "        chart.add_series({'values': '='+shname+'!O'+str(4+i*ntimes)+':O'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                         'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "    chart.set_legend({'position': 'bottom'})\n",
    "    sheet.insert_chart('A'+str(ncat+6+18), chart)\n",
    "\n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "var_stat=pd.merge(Gini_varsm, scale, on=variable_name)\n",
    "var_stat=var_stat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segments\n",
    "n_groups=3\n",
    "sc=scored_all.copy()\n",
    "sc['rank'] = sc[score_name].rank()\n",
    "sc['grouping'] = round(sc['rank']*(n_groups-1)/(len(sc[score_name])+1))\n",
    "sss=pd.DataFrame(sc.groupby(['grouping']).agg({\n",
    "    score_name:['min','max'],\n",
    "    target_name:['mean','count','sum'],\n",
    "    event_value: ['sum'],\n",
    "    all_value:['sum']\n",
    "}))\n",
    "sss=sss.reset_index()\n",
    "sss.columns=['Segment','Min score','Max score',event_rate_name,all_name,event_name,event_value,all_value]\n",
    "sss[event_rate_name_value]=sss[event_value]/sss[all_value]\n",
    "sum_n=sss[all_name].sum()\n",
    "sum_v=sss[all_value].sum()\n",
    "sss[share_name]=sss[all_name]/sum_n\n",
    "sss[share_name_value]=sss[all_value]/sum_v\n",
    "sss=sss[['Segment','Min score','Max score',event_rate_name,all_name,share_name,event_name,event_rate_name_value,event_value,all_value,share_name_value]]\n",
    "\n",
    "\n",
    "ulist=np.unique(sc[time_report_name])\n",
    "sss_time=pd.DataFrame()\n",
    "for t in range(len(ulist)):\n",
    "    ssst=pd.DataFrame()\n",
    "    scored_sub=sc[sc[time_report_name]==ulist[t]].copy()\n",
    "    ssst=pd.DataFrame(scored_sub.groupby(['grouping']).agg({\n",
    "    target_name:['mean','count','sum'],\n",
    "    event_value: ['sum'],\n",
    "    all_value:['sum']\n",
    "    }))\n",
    "    ssst=ssst.reset_index()\n",
    "    ssst.columns=['Segment',event_rate_name,all_name,event_name,event_value,all_value]\n",
    "    ssst[event_rate_name_value]=ssst[event_value]/ssst[all_value]\n",
    "    ssst[time_report_name]=ulist[t]\n",
    "    sum_n=ssst[all_name].sum()\n",
    "    sum_v=ssst[all_value].sum()\n",
    "    ssst[share_name]=ssst[all_name]/sum_n\n",
    "    ssst[share_name_value]=ssst[all_value]/sum_v\n",
    "    ssst=ssst[['Segment',time_report_name,event_rate_name,all_name,share_name,event_name,event_rate_name_value,event_value,all_value,share_name_value]]\n",
    "    sss_time=sss_time.append(ssst,ignore_index=True, sort=False)\n",
    "\n",
    "    \n",
    "sss_time=sss_time.sort_values(by=['Segment',time_report_name])\n",
    "\n",
    "#sss\n",
    "sss_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segments report\n",
    "startrow=2\n",
    "\n",
    "# Dump Pandas DataFrame to Excel sheet\n",
    "writer = pd.ExcelWriter('Segments'+str(n_groups)+'_report.xlsx', engine='xlsxwriter')\n",
    "\n",
    "\n",
    "sss2=pd.DataFrame(grp_all[time_report_name].value_counts())\n",
    "ntimes=sss2.shape[0]\n",
    "\n",
    "shname='Numbers'\n",
    "ssse=sss[['Segment','Min score','Max score',event_rate_name,share_name,all_name,event_name]]\n",
    "sss_timee=sss_time[['Segment',time_report_name,event_rate_name,share_name,all_name,event_name]]\n",
    "ssse.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False)\n",
    "sss_timee.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False, startcol=8)\n",
    "book = writer.book\n",
    "sheet = writer.sheets[shname]\n",
    "bold = book.add_format({'bold': True, 'size': 24})\n",
    "#sheet.write('A1', variable_name+': '+feature, bold)\n",
    "bold = book.add_format({'bold': True, 'size': 12})\n",
    "boldp = book.add_format({'bold': True, 'size': 12, 'num_format': '0.0%'})\n",
    "\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "sheet.set_column('E:E', None, format1)\n",
    "sheet.set_column('D:D', None, format1)\n",
    "\n",
    "sheet.write_formula('E'+str(startrow+n_groups+2), '=SUM(E4:E'+str(startrow+n_groups+1)+')', boldp)\n",
    "sheet.write_formula('F'+str(startrow+n_groups+2), '=SUM(F4:F'+str(startrow+n_groups+1)+')', bold)\n",
    "sheet.write_formula('G'+str(startrow+n_groups+2), '=SUM(G4:G'+str(startrow+n_groups+1)+')', bold)\n",
    "\n",
    "#format1 = workbook.add_format({'num_format': '#,##0.00'})\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "sheet.set_column('K:K', None, format1)\n",
    "sheet.set_column('L:L', None, format1)\n",
    "\n",
    "    # Chart\n",
    "chart = book.add_chart({'type': 'line'})\n",
    "chart.set_title({'name': event_rate_name})\n",
    "chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "for i in range(n_groups):\n",
    "    chart.add_series({'values': '='+shname+'!K'+str(4+i*ntimes)+':K'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                     'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "chart.set_legend({'position': 'bottom'})\n",
    "sheet.insert_chart('A'+str(n_groups+6), chart)\n",
    "\n",
    "\n",
    "chart = book.add_chart({'type': 'line'})\n",
    "chart.set_title({'name': share_name})\n",
    "chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "for i in range(n_groups):\n",
    "    chart.add_series({'values': '='+shname+'!L'+str(4+i*ntimes)+':L'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                     'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "chart.set_legend({'position': 'bottom'})\n",
    "sheet.insert_chart('A'+str(n_groups+6+18), chart)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shname='Balances'\n",
    "ssse=sss[['Segment','Min score','Max score',event_rate_name_value,share_name_value,all_value,event_value]]\n",
    "sss_timee=sss_time[['Segment',time_report_name,event_rate_name_value,share_name_value,all_value,event_value]]\n",
    "ssse.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False)\n",
    "sss_timee.to_excel(writer, sheet_name=shname, startrow=startrow, index_label=None, index=False, startcol=8)\n",
    "book = writer.book\n",
    "sheet = writer.sheets[shname]\n",
    "bold = book.add_format({'bold': True, 'size': 24})\n",
    "#sheet.write('A1', variable_name+': '+feature, bold)\n",
    "bold = book.add_format({'bold': True, 'size': 12})\n",
    "boldp = book.add_format({'bold': True, 'size': 12, 'num_format': '0.0%'})\n",
    "\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "sheet.set_column('E:E', None, format1)\n",
    "sheet.set_column('D:D', None, format1)\n",
    "\n",
    "sheet.write_formula('E'+str(startrow+n_groups+2), '=SUM(E4:E'+str(startrow+n_groups+1)+')', boldp)\n",
    "sheet.write_formula('F'+str(startrow+n_groups+2), '=SUM(F4:F'+str(startrow+n_groups+1)+')', bold)\n",
    "sheet.write_formula('G'+str(startrow+n_groups+2), '=SUM(G4:G'+str(startrow+n_groups+1)+')', bold)\n",
    "\n",
    "#format1 = workbook.add_format({'num_format': '#,##0.00'})\n",
    "format1 = book.add_format({'num_format': '0.0%'})\n",
    "sheet.set_column('K:K', None, format1)\n",
    "sheet.set_column('L:L', None, format1)\n",
    "\n",
    "    # Chart\n",
    "chart = book.add_chart({'type': 'line'})\n",
    "chart.set_title({'name': event_rate_name_value})\n",
    "chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "for i in range(n_groups):\n",
    "    chart.add_series({'values': '='+shname+'!K'+str(4+i*ntimes)+':K'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                     'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "chart.set_legend({'position': 'bottom'})\n",
    "sheet.insert_chart('A'+str(n_groups+6), chart)\n",
    "\n",
    "\n",
    "chart = book.add_chart({'type': 'line'})\n",
    "chart.set_title({'name': share_name_value})\n",
    "chart.set_x_axis({'name': '='+shname+'!J3', 'num_font':  {'rotation': 45}})\n",
    "for i in range(n_groups):\n",
    "    chart.add_series({'values': '='+shname+'!L'+str(4+i*ntimes)+':L'+str(4+(i+1)*ntimes-1), 'name': '='+shname+'!I'+str(4+i*ntimes),\n",
    "                     'categories': '='+shname+'!J'+str(4)+':J'+str(4+ntimes-1)})\n",
    "chart.set_legend({'position': 'bottom'})\n",
    "sheet.insert_chart('A'+str(n_groups+6+18), chart)\n",
    "\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut_off.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut-off point\n",
    "\n",
    "apr=0.18\n",
    "lgd=0.55\n",
    "provision=0\n",
    "\n",
    "def f(x):\n",
    "    if x[target_name]==1:\n",
    "        return -x['app_loan_amount']*lgd\n",
    "    else:\n",
    "        return x['app_n_installments']*x['installment']+x['app_loan_amount']*(provision-1)\n",
    "\n",
    "\n",
    "cut_off=cal_scored.copy()\n",
    "cut_off=pd.merge(cut_off, df_notempty[[id_row,'app_loan_amount','app_n_installments']], on=id_row)\n",
    "cut_off['installment']=cut_off['app_loan_amount']*(apr/12)*((1+apr/12)**cut_off['app_n_installments'])/(((1+apr/12)**cut_off['app_n_installments'])-1)\n",
    "cut_off['Profit'] = cut_off.apply(f, axis=1)\n",
    "cut_off.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr=pd.concat([pd.DataFrame.from_dict(cut_off.groupby('PD')['Profit'].sum()),pd.DataFrame.from_dict(cut_off.groupby('PD')['Profit'].count())],axis=1)\n",
    "agrcum=agr.cumsum()\n",
    "maxobs=agrcum.iloc[:,1].max()\n",
    "agrcum['ar']=agrcum.iloc[:,1]/maxobs\n",
    "maxprofit=agrcum.iloc[:,0].max()\n",
    "agrcum['ProfitCum']=agrcum.iloc[:,0]\n",
    "argcumsort=agrcum.sort_values(by='PD',ascending=True)\n",
    "argcumsort=argcumsort.reset_index()\n",
    "argcumsort.columns=['PD','Profit','NCum','AR','ProfitCum']\n",
    "argcumsort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(argcumsort['PD'], argcumsort['ProfitCum'])\n",
    "plt.plot(argcumsort['AR'], argcumsort['ProfitCum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argcumsort2=argcumsort.copy()\n",
    "argcumsort2=argcumsort2.sort_values(by='ProfitCum',ascending=False)\n",
    "argcumsort2=argcumsort2.reset_index()\n",
    "print(argcumsort2['ProfitCum'][0])\n",
    "argcumsort2.head()\n",
    "#896 902,65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring code\n",
    "\n",
    "vars_sc=list(pd.unique(Scorecard[variable_name]))\n",
    "\n",
    "vars_num=Scorecard[Scorecard[type_name]=='INT'].copy()\n",
    "vars_sc_num=list(pd.unique(vars_num[variable_name]))\n",
    "\n",
    "vars_nom=Scorecard[Scorecard[type_name]=='NOM'].copy()\n",
    "vars_sc_nom=list(pd.unique(vars_nom[variable_name]))\n",
    "\n",
    "with open('scoring_code.sas','w') as f:\n",
    "    f.write('proc sql; \\n')\n",
    "    f.write('create table  &zbior._score as \\n')\n",
    "    f.write('select indataset.*  \\n')\n",
    "    for var in vars_sc_num:\n",
    "        scv=Scorecard[Scorecard[variable_name]==var].copy()\n",
    "        scv=scv.reset_index()\n",
    "        f.write(', case \\n')\n",
    "        for i in range(scv.shape[0]):\n",
    "            war=scv[condition_name][i]\n",
    "            score=scv[score_name][i]\n",
    "            if war.count('<')==2:\n",
    "                f.write('when '+war.rsplit('<',1)[0]+' and '+war.rsplit('<=',1)[1]+' then '+str(score)+' \\n')\n",
    "            if war.count('<')==1 and war.count('<>')==0:\n",
    "                f.write('when '+war+' then '+str(score)+' \\n')\n",
    "            if war.count('= '+symbol_missing)==1:\n",
    "                f.write('when '+war.split(' ')[0]+' is null then '+str(score)+' \\n')\n",
    "            if war.count('<> '+symbol_missing)==1:\n",
    "                f.write('when '+war.split(' ')[0]+' is not null then '+str(score)+' \\n')\n",
    "        score=scv[score_name][0]\n",
    "        f.write('else '+str(score)+' end as PSC_'+var+' \\n')\n",
    "        f.write(' \\n')\n",
    "        \n",
    "    for var in vars_sc_nom:\n",
    "        scv=Scorecard[Scorecard[variable_name]==var].copy()\n",
    "        scv=scv.reset_index()\n",
    "        f.write(', case \\n')\n",
    "        index_other=0;\n",
    "        for i in range(scv.shape[0]):\n",
    "            war=scv[condition_name][i]\n",
    "            score=scv[score_name][i]\n",
    "            if war.count(',')==0 and war.count(symbol_other)==0:\n",
    "                f.write('when '+var+' in ('+\"'\"+war+\"'\"+') then '+str(score)+' \\n')\n",
    "            if war.count(',')>0 and war.count(symbol_other)==0:\n",
    "                f.write('when '+var+' in ('+\"'\"+war.split(', ')[0]+\"'\")\n",
    "                for j in range(war.count(',')):\n",
    "                    f.write(', '+\"'\"+war.split(', ')[j+1]+\"'\")\n",
    "                f.write(') then '+str(score)+' \\n')\n",
    "            if war.count(symbol_other)==1:\n",
    "                index_other=i\n",
    "        score=scv[score_name][index_other]\n",
    "        f.write('else '+str(score)+' end as PSC_'+var+' \\n')\n",
    "        f.write(' \\n')\n",
    "    \n",
    "    f.write('/* , 1/(1+exp(-('+str(model_cal.params[0])+'*(0.0')\n",
    "    for var in vars_sc:            \n",
    "        f.write('+ calculated PSC_'+var)\n",
    "    f.write(')+('+str(model_cal.params[1])+')))) as '+prob_event+' */ \\n')\n",
    "    f.write(' \\n')\n",
    "    \n",
    "    f.write(', 0.0 \\n')\n",
    "    for var in vars_sc:            \n",
    "        f.write('+ calculated PSC_'+var+' ')\n",
    "    f.write(' as SCORECARD_POINTS \\n')\n",
    "    \n",
    "    f.write(' \\n')\n",
    "    f.write('from &zbior as indataset; \\n')\n",
    "    f.write('quit; \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install varclushi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install factor_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable clustering\n",
    "from varclushi import VarClusHi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo1_df.drop('quality',axis=1,inplace=True)\n",
    "demo1_vc = VarClusHi(logit_train[vars_selected],maxeigval2=0.1,maxclus=None)\n",
    "demo1_vc.varclus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Variable_Clustering.xlsx', engine='xlsxwriter')\n",
    "\n",
    "demo1_vc.info.to_excel(writer, sheet_name='Info', index_label=None, index=False)\n",
    "demo1_vc.rsquare.to_excel(writer, sheet_name='Clusters', index_label=None, index=False)\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI Explainable artificial intelligence\n",
    "\n",
    "features=list(result['Efects'][variable_name].copy())\n",
    "features.remove('Intercept')\n",
    "\n",
    "# X=logit_train[features]\n",
    "# y=logit_train[target_name]\n",
    "\n",
    "X=scored_train[features]\n",
    "y=scored_train[target_name]\n",
    "\n",
    "#import shap\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X,y)\n",
    "\n",
    "for i,v in enumerate(features):\n",
    "#     logmodel.coef_[0][i]=result['Efects']['Estimation'][i]\n",
    "    logmodel.coef_[0][i]=1.0\n",
    "    \n",
    "\n",
    "logmodel.intercept_=0.0\n",
    "\n",
    "#explainer=shap.LinearExplainer(logmodel,X)\n",
    "#shap_values=explainer(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(logmodel.coef_,logmodel.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values,X,plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values,X,plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.waterfall_plot(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.initjs()\n",
    "#shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma=0\n",
    "for v in features:\n",
    "    print(v, X[v][0])\n",
    "    suma=suma+X[v][0]\n",
    "print('Score',suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in features:\n",
    "    a=X[v][0]-X[v].mean()\n",
    "    print(v, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS=X.copy()\n",
    "XS['sum']=0.0\n",
    "for v in features:\n",
    "   XS['sum']=XS['sum']+X[v]\n",
    "\n",
    "XS['sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XShap=pd.DataFrame(shap_values.values)\n",
    "#XShap['sum']=0.0\n",
    "#for i,v in enumerate(features):\n",
    "#    XShap['sum']=XShap['sum']+XShap[i]\n",
    "    \n",
    "#XShap['sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=shap_values.base_values.mean()+XShap['sum'].mean()\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'gini': [gini],\n",
    "    'profit': [argcumsort2['ProfitCum'][0]],\n",
    "    'description': [description_results]\n",
    "}\n",
    "# Make data frame of above data\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# append data frame to CSV file\n",
    "df.to_csv('results.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(\"results.csv\")\n",
    "df_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
